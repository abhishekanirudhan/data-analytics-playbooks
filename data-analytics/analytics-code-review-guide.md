# Analytics Code Review Guidelines\n\nComprehensive code review standards for SQL, Python, dbt, and analytics code. Ensures quality, maintainability, and team knowledge sharing across all analytics development.\n\n## üéØ Code Review Objectives\n\n### **Quality Assurance**\n- Catch bugs and logic errors before production\n- Ensure code follows team standards and best practices\n- Validate performance and scalability considerations\n- Verify security and compliance requirements\n\n### **Knowledge Sharing**\n- Spread domain knowledge across team members\n- Share new techniques and approaches\n- Maintain consistent coding patterns\n- Onboard new team members effectively\n\n### **Continuous Improvement**\n- Identify opportunities for refactoring\n- Suggest performance optimizations\n- Recommend additional testing\n- Enhance documentation quality\n\n## üìã Review Process\n\n### **When to Request Reviews**\n- All production code changes\n- New data models or transformations\n- Significant logic modifications\n- Performance-critical changes\n- Security-sensitive code\n\n### **Review Requirements**\n- **Minimum 2 reviewers** for production changes\n- **At least 1 senior reviewer** for architectural changes\n- **Domain expert review** for business logic changes\n- **Security review** for access control changes\n\n### **Review Timeline**\n- **Target response time**: 24 hours for initial review\n- **Maximum review time**: 3 business days\n- **Urgent changes**: Same-day review (with prior notice)\n\n## üîç Review Checklist\n\n### **General Code Quality**\n- [ ] Code is readable and well-structured\n- [ ] Variable and function names are descriptive\n- [ ] Complex logic is properly commented\n- [ ] No hardcoded values (use configuration)\n- [ ] Error handling is appropriate\n- [ ] Code follows DRY principle (Don't Repeat Yourself)\n\n### **SQL Code Review**\n- [ ] Queries are optimized for performance\n- [ ] Appropriate indexes are considered\n- [ ] JOINs are efficient and necessary\n- [ ] WHERE clauses filter data early\n- [ ] No SELECT * in production code\n- [ ] CTEs are used for complex logic\n- [ ] Formatting follows team standards\n\n### **Python Code Review**\n- [ ] Functions have docstrings\n- [ ] Type hints are used where appropriate\n- [ ] Error handling includes specific exceptions\n- [ ] No unused imports or variables\n- [ ] Security best practices followed\n- [ ] Unit tests are comprehensive\n- [ ] Code follows PEP 8 standards\n\n### **dbt Code Review**\n- [ ] Models have proper descriptions\n- [ ] Tests are defined for all models\n- [ ] Macros are documented and reusable\n- [ ] Incremental models handle late-arriving data\n- [ ] Source freshness is configured\n- [ ] Materialization strategy is appropriate\n\n## üìù SQL Review Standards\n\n### **Performance Guidelines**\n```sql\n-- ‚úÖ GOOD: Filter early, use specific columns\nWITH recent_orders AS (\n    SELECT \n        customer_id,\n        order_date,\n        order_amount\n    FROM orders\n    WHERE order_date >= '2024-01-01'\n)\nSELECT \n    customer_id,\n    COUNT(*) as order_count,\n    SUM(order_amount) as total_amount\nFROM recent_orders\nGROUP BY customer_id;\n\n-- ‚ùå BAD: Late filtering, SELECT *\nSELECT \n    customer_id,\n    COUNT(*) as order_count,\n    SUM(order_amount) as total_amount\nFROM (\n    SELECT *\n    FROM orders\n) filtered\nWHERE order_date >= '2024-01-01'\nGROUP BY customer_id;\n```\n\n### **Readability Standards**\n```sql\n-- ‚úÖ GOOD: Clear formatting, descriptive names\nWITH customer_order_summary AS (\n    SELECT\n        c.customer_id,\n        c.customer_name,\n        COUNT(o.order_id) as total_orders,\n        SUM(o.order_amount) as total_revenue,\n        AVG(o.order_amount) as avg_order_value\n    FROM customers c\n    LEFT JOIN orders o\n        ON c.customer_id = o.customer_id\n        AND o.order_date >= '2024-01-01'\n    GROUP BY \n        c.customer_id,\n        c.customer_name\n),\n\ncustomer_segments AS (\n    SELECT\n        *,\n        CASE\n            WHEN total_revenue >= 10000 THEN 'VIP'\n            WHEN total_revenue >= 5000 THEN 'Premium'\n            WHEN total_revenue >= 1000 THEN 'Standard'\n            ELSE 'Basic'\n        END as customer_segment\n    FROM customer_order_summary\n)\n\nSELECT * FROM customer_segments;\n\n-- ‚ùå BAD: Poor formatting, unclear logic\nselect c.customer_id,c.customer_name,count(o.order_id) as total_orders,sum(o.order_amount) as total_revenue,case when sum(o.order_amount)>=10000 then 'VIP' when sum(o.order_amount)>=5000 then 'Premium' when sum(o.order_amount)>=1000 then 'Standard' else 'Basic' end as customer_segment from customers c left join orders o on c.customer_id=o.customer_id and o.order_date>='2024-01-01' group by c.customer_id,c.customer_name;\n```\n\n### **Common SQL Issues to Flag**\n```sql\n-- ‚ùå Potential Cartesian product\nSELECT *\nFROM customers c, orders o\nWHERE c.created_date = o.order_date;  -- Should use proper JOIN\n\n-- ‚ùå Non-deterministic ordering\nSELECT customer_id, order_amount\nFROM orders\nLIMIT 10;  -- Missing ORDER BY\n\n-- ‚ùå Inefficient subquery\nSELECT *\nFROM customers\nWHERE customer_id IN (\n    SELECT customer_id FROM orders  -- Should add DISTINCT\n);\n\n-- ‚ùå Missing NULL handling\nSELECT \n    customer_id,\n    order_date,\n    DATEDIFF(CURRENT_DATE, order_date) as days_since_order  -- NULL if order_date is NULL\nFROM orders;\n```\n\n## üêç Python Review Standards\n\n### **Code Organization**\n```python\n# ‚úÖ GOOD: Clear structure, proper imports\nfrom typing import List, Dict, Optional\nimport pandas as pd\nfrom datetime import datetime\n\nfrom src.config import DATABASE_CONFIG\nfrom src.utils.database import DatabaseConnector\nfrom src.utils.logging import get_logger\n\nlogger = get_logger(__name__)\n\nclass CustomerAnalytics:\n    \"\"\"Customer analytics and segmentation logic.\"\"\"\n    \n    def __init__(self, db_connector: DatabaseConnector):\n        self.db = db_connector\n        \n    def calculate_customer_ltv(\n        self, \n        customer_ids: List[str],\n        start_date: Optional[datetime] = None\n    ) -> Dict[str, float]:\n        \"\"\"Calculate customer lifetime value for given customers.\n        \n        Args:\n            customer_ids: List of customer IDs to analyze\n            start_date: Optional start date for calculation\n            \n        Returns:\n            Dictionary mapping customer_id to calculated LTV\n            \n        Raises:\n            ValueError: If customer_ids is empty\n            DatabaseError: If query execution fails\n        \"\"\"\n        if not customer_ids:\n            raise ValueError(\"customer_ids cannot be empty\")\n            \n        try:\n            query = self._build_ltv_query(customer_ids, start_date)\n            results = self.db.execute_query(query)\n            return {row['customer_id']: row['ltv'] for row in results}\n        except Exception as e:\n            logger.error(f\"Failed to calculate LTV: {e}\")\n            raise\n```\n\n### **Error Handling**\n```python\n# ‚úÖ GOOD: Specific exception handling\ndef extract_customer_data(api_endpoint: str) -> pd.DataFrame:\n    \"\"\"Extract customer data from API.\"\"\"\n    try:\n        response = requests.get(api_endpoint, timeout=30)\n        response.raise_for_status()\n        \n        data = response.json()\n        return pd.DataFrame(data['customers'])\n        \n    except requests.exceptions.Timeout:\n        logger.error(f\"Timeout accessing {api_endpoint}\")\n        raise\n    except requests.exceptions.HTTPError as e:\n        logger.error(f\"HTTP error from {api_endpoint}: {e}\")\n        raise\n    except KeyError as e:\n        logger.error(f\"Missing expected key in API response: {e}\")\n        raise ValueError(f\"Invalid API response format: missing {e}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error extracting data: {e}\")\n        raise\n\n# ‚ùå BAD: Generic exception handling\ndef extract_customer_data(api_endpoint: str) -> pd.DataFrame:\n    try:\n        response = requests.get(api_endpoint)\n        data = response.json()\n        return pd.DataFrame(data['customers'])\n    except:\n        return pd.DataFrame()  # Silent failure\n```\n\n## üèóÔ∏è dbt Review Standards\n\n### **Model Documentation**\n```yaml\n# ‚úÖ GOOD: Comprehensive documentation\nversion: 2\n\nmodels:\n  - name: customer_lifetime_value\n    description: |\n      Customer lifetime value calculation based on historical purchase behavior.\n      Updated daily with latest transaction data. Used for customer segmentation\n      and marketing campaign targeting.\n    \n    config:\n      materialized: table\n      indexes:\n        - columns: ['customer_id']\n          unique: true\n    \n    columns:\n      - name: customer_id\n        description: \"Unique customer identifier\"\n        tests:\n          - unique\n          - not_null\n          - relationships:\n              to: ref('dim_customers')\n              field: customer_id\n      \n      - name: ltv_amount\n        description: \"Calculated lifetime value in USD\"\n        tests:\n          - not_null\n          - dbt_utils.accepted_range:\n              min_value: 0\n              max_value: 100000\n      \n      - name: calculation_date\n        description: \"Date when LTV was calculated\"\n        tests:\n          - not_null\n```\n\n### **Model Structure**\n```sql\n-- ‚úÖ GOOD: Clear structure with CTEs\n{{ config(materialized='table') }}\n\nwith customer_orders as (\n    select\n        customer_id,\n        count(*) as total_orders,\n        sum(order_amount) as total_revenue,\n        avg(order_amount) as avg_order_value,\n        min(order_date) as first_order_date,\n        max(order_date) as last_order_date\n    from {{ ref('fact_orders') }}\n    where order_status = 'completed'\n    group by customer_id\n),\n\ncustomer_lifetime_months as (\n    select\n        customer_id,\n        total_orders,\n        total_revenue,\n        avg_order_value,\n        first_order_date,\n        last_order_date,\n        \n        -- Calculate customer lifetime in months\n        greatest(\n            {{ dbt_utils.datediff('first_order_date', 'last_order_date', 'month') }},\n            1\n        ) as lifetime_months\n    from customer_orders\n),\n\nfinal as (\n    select\n        customer_id,\n        total_orders,\n        total_revenue,\n        avg_order_value,\n        lifetime_months,\n        \n        -- LTV calculation\n        round(\n            (total_revenue / lifetime_months) * 24,  -- 24 month projection\n            2\n        ) as ltv_amount,\n        \n        current_timestamp as calculation_date\n    from customer_lifetime_months\n)\n\nselect * from final\n```\n\n## üìä Review Feedback Guidelines\n\n### **Constructive Feedback Examples**\n\n#### **‚úÖ Good Feedback**\n```\n\"Consider adding an index on the order_date column since this query \nfilters on it frequently. This could improve performance for the daily \nreports that use this model.\n\nSuggestion:\nconfig:\n  indexes:\n    - columns: ['order_date']\n\"\n```\n\n#### **‚úÖ Good Feedback with Code**\n```\n\"The CASE statement for customer segmentation could be extracted into \na macro for reusability across other models.\n\nSuggestion - create macro:\n{% macro customer_segment(revenue_column) %}\n  case\n    when {{ revenue_column }} >= 10000 then 'VIP'\n    when {{ revenue_column }} >= 5000 then 'Premium'\n    when {{ revenue_column }} >= 1000 then 'Standard'\n    else 'Basic'\n  end\n{% endmacro %}\n\nThen use: {{ customer_segment('total_revenue') }}\n\"\n```\n\n#### **‚ùå Poor Feedback**\n```\n\"This code is bad.\"\n\"Why did you do it this way?\"\n\"This won't work.\"\n```\n\n### **Review Response Guidelines**\n\n#### **For Review Authors**\n- Be open to feedback and suggestions\n- Ask clarifying questions if feedback is unclear\n- Explain your reasoning for design decisions\n- Update code based on valid feedback\n- Thank reviewers for their time and insights\n\n#### **For Reviewers**\n- Be specific and constructive\n- Explain the reasoning behind suggestions\n- Provide code examples when helpful\n- Focus on the code, not the person\n- Acknowledge good practices you see\n\n## üõ†Ô∏è Tools & Automation\n\n### **Automated Checks**\n```yaml\n# .github/workflows/code-quality.yml\nname: Code Quality Checks\n\non: [pull_request]\n\njobs:\n  sql-lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: SQL Lint\n        run: |\n          pip install sqlfluff\n          sqlfluff lint models/ --dialect postgres\n  \n  python-quality:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Python Quality Checks\n        run: |\n          pip install black flake8 mypy\n          black --check .\n          flake8 .\n          mypy src/\n  \n  dbt-checks:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: dbt Quality Checks\n        run: |\n          pip install dbt-core\n          dbt parse\n          dbt test --select test_type:generic\n```\n\n### **Review Templates**\n```markdown\n## Code Review Checklist\n\n### Functionality\n- [ ] Code accomplishes the intended purpose\n- [ ] Business logic is correct\n- [ ] Edge cases are handled appropriately\n- [ ] Error handling is comprehensive\n\n### Quality\n- [ ] Code is readable and well-documented\n- [ ] Follows team coding standards\n- [ ] No obvious performance issues\n- [ ] Security considerations addressed\n\n### Testing\n- [ ] Appropriate tests are included\n- [ ] Tests cover main functionality and edge cases\n- [ ] Test data is representative\n- [ ] Tests will catch regressions\n\n### Documentation\n- [ ] Code changes are documented\n- [ ] Comments explain complex logic\n- [ ] README updated if needed\n- [ ] Data dictionary updated if needed\n\n## Summary\n[Provide overall assessment and key feedback]\n```\n\n## üìã Implementation Checklist\n\n### **Team Setup**\n- [ ] Establish review process and requirements\n- [ ] Set up code quality automation tools\n- [ ] Create review templates and checklists\n- [ ] Train team on effective review practices\n- [ ] Define escalation process for disagreements\n\n### **Standards Documentation**\n- [ ] Document SQL coding standards\n- [ ] Document Python coding standards\n- [ ] Document dbt modeling standards\n- [ ] Create examples of good and bad practices\n- [ ] Establish review timeline expectations\n\n### **Continuous Improvement**\n- [ ] Track review metrics (time, quality, feedback)\n- [ ] Regularly update standards based on learnings\n- [ ] Share common issues and solutions\n- [ ] Recognize good review practices\n- [ ] Gather team feedback on review process\n\n---\n\n**Last Updated**: 2025-01-19  \n**Owner**: Data Engineering Team